<!--
 * @Descripttion: 
 * @version: 
 * @Author: sch
 * @Date: 2022-03-07 13:33:58
 * @LastEditors: sch
 * @LastEditTime: 2022-03-07 14:46:05
-->
# What is `torch.nn.Dropout(possibility: float)`?

<font color="orange" size="4">

Definition
----------

- Dropout is a machine learning technique where you remove units in neural network to simulate training large numbers of architectures simultaneously. 
- It can `reduce the chances of overfitting` during training.
- 
  
</font>

<font color="red" size="4">

How to use?
-----------
1. See Demo 1 and Demo 2

</font>


# 1. Add Dropout to a PyTorch Model
<font color="orange" size="4">

- Adding dropout to your PyTorch Models is straightforward with the `torch.nn.Dropout()` class, which takes in dropout rate - the possibility of a neuron being deactivated - as a parameter.

</font>

```python
import torch.nn as nn

self.dropout = nn.Dropout(0.25)
```


# 2. Demo 1 
```python
import torch
import torch.nn as nn


N = 50
noise = 0.3


# generate the train data
X_train = torch.unsqueeze(torch.linspace(-1, 1, 50), 1)
y_train =  X_train + noise * torch.normal(torch.zeros(N, 1), torch.ones(N ,1))

# generate the test data
X_test = torch.unsqueeze(torch.linspace(-1, 1, N), 1)
y_test = X_test + noise * torch.normal(torch.zeros(N, 1), torch.ones(N, 1))


N_h = 100 # hiddeen nodes


model = nn.Sequential(
    nn.Linear(1, N_h),
    nn.ReLU(),
    nn.Linear(N_h. N_h),
    nn.ReLU(),
    nn.Linear(N_h, 1)
)


model_dropout = nn.Sequential(
    nn.Linear(1, N_h),
    nn.Dropout(0.5), # 50% possibility
    nn.ReLU(),
    nn.Linear(N_h, N_h),
    nn.Dropout(0.2),
    nn.ReLU(),
    nn.Linear(N_h, 1)
)
```


# 3. Demo 2
## 3.1. Model

```python
import torch.nn as nn

class Net(nn.Module):
    def __init__(self, input_shape=(3, 32, 32)):
        super(Net, self).__init__()

        self.conv1 = nn.Conv2d(3, 32, 3)
        self.conv2 = nn.Conv2d(32, 64, 3)
        self.conv3 = nn.Conv2d(64, 128, 3)

        self.pool = nn.MaxPool2d(2, 2)

        n_size = self._get_conv_output(input_shape)
        
        self.fc1 = nn.Linear(n_size, 512)
        self.fc2 = nn.Linear(512, 10)

        # Define the proportion or neurons to dropout
        self.dropout = nn.Dropout(0.25)

    
    def forward(self, x):
        x = self._forward_features(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        # Apply dropout
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

## 3.2. `wandb.log()`: Track performance of model
<font color="orange" size="4">

- By using `wandb.log()` in your training function, you automatically track the performance of your model.

</font>

```python
def train(model, train_loader, optimizer, criterion, num_epochs, steps_per_epoch=20):
    # Log gradients and model parameters
    wandb.watch(model)

    # loop over the data iterator, and feed the inputs to the network and adjust the weights.
    for batch_idx, (inputs, target) in enumerate(train_loader):
        # ...

        acc = round((train_correct / train_total) * 100, 2)
        # Log metrics to visuallize performance 
        wandb.log({"Train Loss": train_loss/train_total, "Train_Accuracy": acc})
```


# 4. `torch.unsqueeze(一维tensor, 1)`

```python
import torch


X_train = torch.linspace(-1, 1, 50)
print(X_train.shape)

X_train = torch.unsqueeze(X_train, 1)
print(X_train.shape)
```
Output:
```shell
torch.Size([50])
torch.Size([50, 1])
```

# 5. `torch.normal(mean:torch.Tensor, std:torch.Tensor)`
```python
import torch


N = 5
X_train = torch.normal(torch.zeros(N, 1), torch.ones(N, 1))
print(X_train)
```
Output:
```shell
tensor([[-0.0874],
        [-0.0580],
        [ 0.1265],
        [ 1.2484],
        [ 0.3961]])
```