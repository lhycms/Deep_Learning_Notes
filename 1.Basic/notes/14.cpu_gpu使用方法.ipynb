{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. `cuda` 与 `cpu` 的相关信息和操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 查看 PyTorch 版本"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. macOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. MCloud2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. slurm 提交脚本 -- `deeplearning.slurm`\n",
    "```bash\n",
    "#!/bin/sh\n",
    "#SBATCH --partition=3080ti\n",
    "#SBATCH --job-name=deeplearning\n",
    "#SBATCH --nodes=1\t\t# Number of nodes\n",
    "#SBATCH --ntasks-per-node=1\t# processes per node: 4;  Note: if gpus-per-task=1, ntasks-per-node==gres=gpu:\n",
    "#SBATCH --gres=gpu:1\t\t# Number of GPUs: 4\n",
    "#SBATCH --gpus-per-task=1\t# Number of GPUs per process\n",
    "\n",
    "module load intel/2020\n",
    "\n",
    "# Suboption 1. My conda: fail to call conda \n",
    "#  source /data/home/liuhanyu/anaconda3/etc/profile.d/conda.sh\n",
    "#  module load cuda/11.6\n",
    "#  conda activate workdir\n",
    "\n",
    "# Suboption 2. /share/app/anaconda3: \n",
    "source /share/app/anaconda3/etc/profile.d/conda.sh\n",
    "module load cuda/11.3\n",
    "conda activate mlff\n",
    "\n",
    "mpirun -np $SLURM_NPROCS python test.py > output \n",
    "```\n",
    "\n",
    "### Step 2. 运行的 Python 脚本 -- `test.py`\n",
    "```python\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "```\n",
    "\n",
    "### Step 3. 提交任务并查看结果\n",
    "```bash\n",
    "$ sbatch deeplearning.slurm\n",
    "$ cat output\n",
    "True\n",
    "1.11.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 查看所有的可用的 cpu 设备的数量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. macOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cpu devices: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"Available cpu devices: {0}\".format(torch.cuda.os.cpu_count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. MCloud2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Python 脚本\n",
    "```python\n",
    "import torch\n",
    "\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "print(\"Cuda version: \", torch.__version__)\n",
    "print(\"Available Cpu devices: \", torch.cuda.os.cpu_count())\n",
    "```\n",
    "\n",
    "### Step 2. 提提交任并查看结果\n",
    "```bash\n",
    "$ sbatch deeplearning.slurm\n",
    "$ cat output\n",
    "Cuda available:  True\n",
    "Cuda version:  1.11.0\n",
    "Available Cpu devices:  28\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 查看所有的可用的 GPU 的数量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. macOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Available GPU devices: \", torch.cuda.device_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. MCloud2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Python 脚本\n",
    "```python\n",
    "import torch\n",
    "\n",
    "\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "print(\"Cuda version: \", torch.__version__)\n",
    "print(\"Available CPU devices: \", torch.cuda.os.cpu_count())\n",
    "print(\"Available GPU devices: \", torch.cuda.device_count())\n",
    "```\n",
    "\n",
    "\n",
    "## Step 2. 提交任务并查看结果\n",
    "```bash\n",
    "# Case 1. \n",
    "#    #SBATCH --ntasks-per-node=1\n",
    "#    #SBATCH --gres=gpu:1\n",
    "$ sbatch deeplearning.slurm\n",
    "$ cat output\n",
    "Cuda available:  True\n",
    "Cuda version:  1.11.0\n",
    "Available CPU devices:  28\n",
    "Available GPU devices:  1\n",
    "\n",
    "# Case 2.\n",
    "#    #SBATCH --ntasks-per-node=4\n",
    "#    #SBATCH --gres=gpu:4\n",
    "$ sbatch deeplearning.slurm\n",
    "$ cat output\n",
    "Cuda available:  True\n",
    "Cuda version:  1.11.0\n",
    "Available CPU devices:  28\n",
    "Available GPU devices:  4\n",
    "Cuda available:  True\n",
    "Cuda version:  1.11.0\n",
    "Available CPU devices:  28\n",
    "Available GPU devices:  4\n",
    "Cuda available:  True\n",
    "Cuda version:  1.11.0\n",
    "Available CPU devices:  28\n",
    "Available GPU devices:  4\n",
    "Cuda available:  True\n",
    "Cuda version:  1.11.0\n",
    "Available CPU devices:  28\n",
    "Available GPU devices:  4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 获取 GPU 设备的名称 (e.g. `\"cuda:0\"`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. MCloud2\n",
    "\n",
    "### Step 1. Python 脚本\n",
    "```python\n",
    "import torch\n",
    "\n",
    "\n",
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "print(\"Cuda version: \", torch.__version__)\n",
    "print(\"Available CPU devices: \", torch.cuda.os.cpu_count())\n",
    "print(\"Available GPU devices: \", torch.cuda.device_count())\n",
    "print(\"GPU device(cuda:0) name: \", torch.cuda.get_device_name(\"cuda:0\"))\n",
    "```\n",
    "\n",
    "### Step 2. 提交任务并查看结果\n",
    "```bash\n",
    "$ sbatch deeplearning.slurm\n",
    "$ cat output\n",
    "Cuda available:  True\n",
    "Cuda version:  1.11.0\n",
    "Available CPU devices:  28\n",
    "Available GPU devices:  1\n",
    "GPU device(cuda:0) name:  NVIDIA GeForce RTX 3080 Ti\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 通过 `torch.device()` 指定设备 (在 MCloud2 上)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. 指定 `\"cpu:0\"` \n",
    "### Step 1. Python 脚本\n",
    "```python\n",
    "import torch \n",
    "\n",
    "cpu_1 = torch.device(\"cpu:0\")\n",
    "print(\"CPU device: {0}: {1}\".format(cpu_1.type, cpu_1.index))\n",
    "```\n",
    "\n",
    "### Step 2. 提交任务并查看结果\n",
    "```bash\n",
    "$ sbatch deeplearning.slurm\n",
    "CPU device: cpu: 0\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. 指定 `\"cuda:0\"` 设备\n",
    "### Step 1. Python 脚本\n",
    "```python\n",
    "import torch \n",
    "\n",
    "gpu_1 = torch.device(\"cuda:0\")\n",
    "print(\"GPU device: {0}: {1}\".format(gpu_1.type, gpu_1.index))\n",
    "```\n",
    "\n",
    "### Step 2. 提交任务并查看结果\n",
    "```bash\n",
    "$ sbatch deeplearning.slurm\n",
    "GPU device: cuda: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. `cpu` 和 `cuda` 设备上的 `Tensor`\n",
    "\n",
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "1. 默认情况下，`tensor` 是在 cpu 设备上创建的，但是可以通过一下方法转移到 gpu 设备上:\n",
    "    - `cpu_tensor.to(<gpu_device>)`: 将 cpu_tensor 拷贝到 GPU 上\n",
    "    - `cpu_tensor.cuda(<gpu_device>)`: 将 cpu_tensor 拷贝到 GPU 上\n",
    "    - `cpu_tensor.copy_(<gpu_tensor>)`: 将gpu上的 gpu_tensor 拷贝到cpu上的 cpu_tensor 中\n",
    "2. `torch.tensor` 和 `torch.Tensor` 的区别是:\n",
    "    - `torch.tensor` 可以通过 `device` 指定gpu设备\n",
    "    - `torch.Tensor` 只能在 cpu 上创建，否则报错。\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. `Tensor` 从 CPU 拷贝到 GPU 上\n",
    "\n",
    "### Step 1. Python 脚本\n",
    "```python\n",
    "import torch \n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 默认创建的 tensor 是在 CPU 上的\n",
    "cpu_tensor = torch.Tensor([\n",
    "                    [1, 4, 7],\n",
    "                    [3, 6, 9],\n",
    "                    [2, 5, 8],\n",
    "                    ])\n",
    "print(cpu_tensor.device)\n",
    "\n",
    "# Way 1\n",
    "gpu_tensor_1 = cpu_tensor.to(device=device)\n",
    "print(\"gpu_tensor_1.device: \", gpu_tensor_1.device)\n",
    "\n",
    "# Way 2\n",
    "gpu_tensor_2 = cpu_tensor.cuda(device=device)\n",
    "print(\"gpu_tensor_2.device: \", gpu_tensor_2.device)\n",
    "\n",
    "# Way 3\n",
    "gpu_tensor_3 = cpu_tensor.copy_(gpu_tensor_2)\n",
    "print(\"gpu_tensor_3.device: \", gpu_tensor_3.device)\n",
    "print(gpu_tensor_3)\n",
    "```\n",
    "\n",
    "### Step 2. 提交任务并查看结果\n",
    "```bash\n",
    "$ sbatch deeplearning.slurm\n",
    "$ cat output\n",
    "cpu                             # Generate tensor in default way.\n",
    "gpu_tensor_1.device:  cuda:0    # Way 1 \n",
    "gpu_tensor_2.device:  cuda:0    # Way 2\n",
    "gpu_tensor_3.device:  cpu       # Way 3\n",
    "tensor([[1., 4., 7.],\n",
    "        [3., 6., 9.],\n",
    "        [2., 5., 8.]])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 直接在 GPU 上创建 `Tensor`\n",
    "### Step 1. Python 脚本\n",
    "```python\n",
    "import torch \n",
    "\n",
    "\n",
    "### Case 1. \n",
    "gpu_tensor_1 = torch.tensor([\n",
    "                    [2, 5, 8],\n",
    "                    [1, 4, 7],\n",
    "                    [3, 6, 9],\n",
    "                    ],\n",
    "                    device=torch.device(\"cuda:0\"))\n",
    "print(gpu_tensor_1.device)\n",
    "\n",
    "### Case 2.\n",
    "gpu_tensor_2 = torch.rand(size=(3, 4), device=torch.device(\"cuda:0\"))\n",
    "print(gpu_tensor_2.device)\n",
    "\n",
    "### Case 3.\n",
    "gpu_tensor_3 = torch.ones(size=(3, 4), device=torch.device(\"cuda:0\"))\n",
    "print(gpu_tensor_3.device)\n",
    "\n",
    "```\n",
    "\n",
    "### Step 2. 提交任务并查看结果\n",
    "```bash\n",
    "$ sbatch deeplearning.slurm\n",
    "$ cat output\n",
    "cuda:0\n",
    "cuda:0\n",
    "cuda:0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workdir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba638ad70ccaeb6ccd331f5570d9b10b0c0ed5153ede80b1237556805ea6ecaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
