# 0. 
<fotn color="steelblue" size="4">

1. Website: https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61

</font>


# 1. Demo 1:间隔 10 个 epoch 保存完整模型
<font color="steelblue" size="4">

1. `torch.save(<model>, <model_path>)`: 存储 `model`
2. `torch.load(<model_path>)`: 载入 `model`

</font>

## 1.1. code
```python
import torch 
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression


# 1. Data Preparation
X_numpy, y_numpy = make_regression(
                            n_samples=100,
                            n_features=1,
                            noise=5,
                            random_state=0
                            )
X = torch.from_numpy(X_numpy.astype(np.float32))
y = torch.from_numpy(y_numpy.astype(np.float32))
y = y.view(y.size()[0], 1)

n_samples, n_features = X.size()


# 2. Model 
input_size= n_features
output_size = 1
model = nn.Linear(input_size, output_size)


# 3. Loss and optimizer
learning_rate = 1e-1
weight_decay = 1e-8
loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(
                        model.parameters(),
                        lr=learning_rate,
                        weight_decay=weight_decay
                        )


# 4. Train Loop
num_epochs = 1000
for epoch in range(num_epochs):
    # Forward: y_pred and loss
    y_pred = model(X)
    loss = loss_function(y_pred, y)
    
    # Backward: gradient
    loss.backward()
    
    # Update
    optimizer.step()
    optimizer.zero_grad()
    
    if (epoch % 10 == 0):
        # 保存模型 -- 整体保存（往往只需要存储关键字）
        torch.save(model, "<your_path>/data/model.pkl")
        # 把 Epoch 和 Loss 输出到 Epoch_Loss 文件中
        with open("<your_path>/data/Epoch_Loss", "a") as f:
            f.write( "Epoch {0:<5d}, Loss = {1}\n".format(epoch, loss) )
    
    
with torch.no_grad():
    y_pred = model(X).numpy().ravel()
    plt.plot(y_pred, y_numpy, "ro")
    plt.plot([-100, 100], [-100, 100], "b")
    plt.savefig("<your_path>/data/train.jpg", dpi=300)
```

## 1.2. Output
```shell
$ ls <your_path>/data/
Epoch_Loss  model.pkl  train.jpg

$ vim Epoch_Loss
Epoch 0    , Loss = 1791.86181640625
Epoch 10   , Loss = 1703.7615966796875
Epoch 20   , Loss = 1620.397705078125
Epoch 30   , Loss = 1541.412353515625
Epoch 40   , Loss = 1465.87939453125
Epoch 50   , Loss = 1393.1480712890625
...
...
...
Epoch 950  , Loss = 28.79935073852539
Epoch 960  , Loss = 28.7640380859375
Epoch 970  , Loss = 28.733352661132812
Epoch 980  , Loss = 28.706703186035156
Epoch 990  , Loss = 28.683603286743164
```