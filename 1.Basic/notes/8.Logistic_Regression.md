<!--
 * @Descripttion: 
 * @version: 
 * @Author: sch
 * @Date: 2022-02-28 17:31:50
 * @LastEditors: sch
 * @LastEditTime: 2022-03-01 22:08:05
-->
# Logistic Regression

# 1. Logistic Regresion Pipeline with PyTorch
<font color="orange" size="4">

1. Design model ( input size, output size, forward pass )
2. Construct `loss` and `optimizer`
3. Training loop
    - `forward pass`: compute the prediction
    - `backward pass`: gradients
    - `updates weights`

</font>

```python
from distutils.log import Log
import torch
import torch.nn as nn
import numpy as np 
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# 0) prepare dataset
bc = datasets.load_breast_cancer()
X, y = bc.data, bc.target

n_samples, n_features = X.shape
input_size = n_features
output_size = 1

X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2,
                                                    random_state=0)

# scale
### scaler 在 X_train 上fit完毕后，直接应用于X_test（千万不要重新在 X_test 上fit）
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# numpy.ndarray -> torch.tensor
X_train = torch.from_numpy(X_train.astype(np.float32))
X_test = torch.from_numpy(X_test.astype(np.float32))
y_train = torch.from_numpy(y_train.astype(np.float32))
y_test = torch.from_numpy(y_test.astype(np.float32))

y_train = y_train.view(-1, 1)
y_test = y_test.view(-1, 1)

# 1) model
class LogisticRegression(nn.Module):

    def __init__(self, input_dim, output_dim):
        super(LogisticRegression, self).__init__()
        self.lin = nn.Linear(input_dim, output_dim)
    
    def forward(self, x):
        # PyTorch 内置的 sigmoid 激活函数。 -- `torch.sigmoid(self.lin)`
        y_predicted = torch.sigmoid(self.lin(x))
        return y_predicted

model = LogisticRegression(input_size, output_size)
    

# 2) loss and optimizer

# `nn.BCELoss()`: Binary cross-entropy loss
learning_rate = 0.01
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)


# 3) training loop
num_epochs = 1000

for epoch in range(1, num_epochs+1):
    y_predicted = model(X_train)
    loss = criterion(y_predicted, y_train)

    loss.backward()

    optimizer.step()

    optimizer.zero_grad()

    if ( epoch % 10 == 0 ):
        [w, b] = model.parameters()
        print("Epoch {0}: weights = {1}, loss = {2}".format(epoch, w[0][0], loss))


with torch.no_grad():
    ### sigmoid function return a value between 0 and 1 :
    ###     - if value > 0.5, we say this belong to 1;
    ###     - if value < 0.5, we say this belong to 0.
    y_predicted = model(X_test)
    # type(y_predicted) = torch.tensor
    y_predicted_cls = y_predicted.round()

    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.size()[0])
    
    print("Accurancy: {0}".format(acc))
```

# 2. `torch.nn.BCELoss()`: Binary Cross Entropy Loss
<font color="red" size="4">

Note
----
1. 所有的 Loss 计算类，都处于 `torch.nn` 模块下。
2. 使用方法：
```python
import torch.nn as nn

for epoch in range(1, n_iters + 1):
    ...
    loss = nn.BCELoss()
    loss(y_predicted, y)
    ...
```

</font>


# 3. `torch.sigmoid(self.lin)`:  Pytorch 内置的 sigmoid 函数
## 3.1. 使用方法 -- 配合 `tensor.round()`
<font color="red" size="4">

Sigmoid function return a value between 0 and 1:
- If `value > 0.5`, we say it belongs to class_1
- If `value < 0.5`, we say it belongs to class_0

</font>

## 3.2. 使用Demo: 自定义 `LogisticRegression`
```python
import torch 
import torch.nn


# 自定义 LogisticeRegression 类
class LogisticRegression(nn.moudle):

    def __init__(self, input_dim, output_dim):
        super(LogisticRegression, self).__init__()
        # 神经网络的每一层都需要设置 输出尺寸、输出尺寸
        self.lin = nn.Linear(input_dim, output_dim)
    
    def forward(self, x):
        y_predicted = torch.sigmoid(self.lin(x))
        return y_predicted
```

## 3.3. 测试集准确度计算方法 -- `tensor.eq(tensor)`
```python
with torch.no_grad():
    y_predicted = model(X_train)
    y_predicted_cls = y_predicted.round()
    accuracy = y_predicted_cls.eq(y_predicted).sum() / y_test.shape[0]
```


# 4. 数据准备
## 4.1. 大致步骤
<font color="orange" size="4">

Steps
-----
1. 得到 `X` 和 `y` 的 numpy.ndarray 形式
2. 计算得到
    - `n_samples`, `n_features`: 训练集的样本数、特征数
    - `input_size`, `output_size`: 神经网路的输入、输出尺寸
3. `train_test_split()`：划分训练集和测试集
4. Scaler
Scaler在`X_train`上fit后，可以直接应用于`X_test`（切忌在`X_test`上重新fit）
- `X_train`
- `X_test`
5. `numpy.ndarray -> torch.tensor`
- `X_train`
- `X_test`
- `y_train`
- `y_test`

</font>