<!--
 * @Descripttion: 
 * @version: 
 * @Author: sch
 * @Date: 2022-02-28 10:30:05
 * @LastEditors: sch
 * @LastEditTime: 2022-02-28 11:49:54
-->
# Gradient Descent with Autograd and Backpropagation

# 1. Linear regression algorithm (最终版)
<font color="gree" size="4">

PyTorch 神经网络搭建方法
---------------------
1. `Prediction`: Pytorch Model
2. `Gradients Computation`: Autograd
3. `Loss Computation`:  PyTorch Loss
4. `Parameter(Weights) updates`: PyTorch Optimizer

</font>


# 2. 逐步改造`Linear Regression`（manually -> torch）

## 2.1. Manually
```word
1. `Prediction`: Manually
2. `Gradients Computation`: Manually
3. `Loss Computation`:  Manually
4. `Parameter(Weights) updates`: Manually
```

```python
import torch
import numpy as np


# f = 2 * x
X = np.array([1, 2, 3, 4], dtype=np.float32)
Y = np.array([2, 4, 6, 8], dtype=np.float32)

w = 0.0

# model prediction
def forward(x):
    return w * x

# loss = MSE (Mean Squared Error)
def loss(y, y_predicted):
    return ( (y_predicted - y)**2 ).mean()

# gradient 
# MSE = 1/N * (w*x - y) ** 2
# dJ/dw = 1/N 2x (w*x - y)
def gradient(x, y, y_predicted):
    return np.dot(2*x, y_predicted-y).mean()


print("Prediction before training: f(5) = {}".format(forward(5)))

# Training
learning_rate = 0.01
n_iters = 10

for epoch in range(n_iters):
    # prediction (forward pass)
    y_pred = forward(X)

    # loss
    l = loss(Y, y_pred)

    # gradients
    dw = gradient(X, Y, y_pred)

    # update weights
    w -= learning_rate * dw


    if epoch % 1 == 0:
        print("Epoch {0}: w = {1}, loss = {2}".format(epoch+1, w, l))

print("Prediction after training: f(5) = {}".format(forward(5)))
```

<font color="red" size="4">

Note
----
1. `forward()` 和 `loss()` 可以保持不变，因为 PyTorch 和 Numpy 可以使用相同的运算语法(`operation syntax`)
2. 权重更新(`Update Weights`)那一步，不需要纳入计算图中(`Computational Graph`)，因此可以使用 `with torch.no_grad():` 上下文背景。

</font>

## 2.2. `Gradient Computation` -- `Autograd`
```word
1. `Prediction`: Manually
2. `Gradients Computation`: Autograd
3. `Loss Computation`:  Manually
4. `Parameter(Weights) updates`: Manually
```

```python
import torch
import numpy as np


# Data Init
X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)

w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)

'''
    forward() and loss() 保持原样：因为 `PyTorch` 和 `Numpy` 可以使用相同的语法
'''

# Model Function
# y = w * x
def forward(x):
    return w * x

# loss
def loss(y, y_predicted):
    return ( (y_predicted - y) ** 2 ).mean()


print("Prediction before training: f(5) = {}".format(forward(5)))

# Training 
learning_rate = 0.01
n_iters = 10

for epoch in range(n_iters):
    # prediction
    y_pred = forward(X)
    
    # loss
    l = loss(Y, y_pred)

    # gradients = backward pass
    l.backward()    # dl/dw
    
    # update weights
    '''
        This operation (update weights) should not be part of our gradient tracking (Computation Graph)
    '''
    with torch.no_grad():
        w -= learning_rate * w.grad

    # zero gradients
    '''
        Whenever we call backward it will write our gradients and accumualte them in the `w.grad`
    '''
    w.grad.zero_()

    if epoch % 1 == 0:
        print("Epoch {0}: w = {1}, loss={2}".format(epoch+1, w, l))
    
print("Prediction after training: f(5) = {}".format(forward(5)))
```