# Training Pipeline: Model, Loss, and Optimizer

# 1. Pipeline
<font color="gree" size="4">

1. Design model ( input, output size, forward pass )
2. Construct `loss` and `optimizer`
3. Training loop
      - `forward pass`: compute the prediction
      - `backward pass`: gradients 
      - `updates weights`

</font>

## 1.1. 
<font color="gree" size="4">

PyTorch 神经网络搭建方法
---------------------
1. `Prediction`: Manually
2. `Gradients Computation`: Autograd
3. `Loss Computation`:  PyTorch Loss
4. `Parameter(Weights) updates`: PyTorch Optimizer

</font>

```python
# 1) Design model ( input, output size, forward pass )
# 2) Construct loss and optimizer
# 3) Training loop
#   - forward pass: compute the prediction
#   - backward pass: gradients 
#   - updates weights
import torch
import torch.nn as nn


# Data Init
X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)

w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)

'''
    forward() and loss() 保持原样：因为 `PyTorch` 和 `Numpy` 可以使用相同的语法
'''

# Model Function
# y = w * x
def forward(x):
    return w * x

print("Prediction before training: f(5) = {}".format(forward(5)))

# Training 
learning_rate = 0.01
n_iters = 100

# This is a callable function
loss = nn.MSELoss()

optimizer = torch.optim.SGD([w], lr=learning_rate)

for epoch in range(n_iters):
    # prediction
    y_pred = forward(X)
    
    # loss
    l = loss(Y, y_pred)

    # gradients = backward pass
    l.backward()    # dl/dw
    
    # update weights
    optimizer.step()

    # zero gradients
    '''
        Whenever we call backward it will write our gradients and accumualte them in the `w.grad`
    '''
    optimizer.zero_grad()

    if epoch % 1 == 0:
        print("Epoch {0}: w = {1}, loss={2}".format(epoch+1, w, l))
    
print("Prediction after training: f(5) = {}".format(forward(5)))
```

## 1.2. PyTorch 神经网络搭建方法
<font color="gree" size="4">

PyTorch 神经网络搭建方法
---------------------
1. `Prediction`: Pytorch Model
2. `Gradients Computation`: Autograd
3. `Loss Computation`:  PyTorch Loss
4. `Parameter(Weights) updates`: PyTorch Optimizer

</font>

```python
# 1) Design model ( input, output size, forward pass )
# 2) Construct loss and optimizer
# 3) Training loop
#   - forward pass: compute the prediction
#   - backward pass: gradients 
#   - updates weights
import torch
import torch.nn as nn


# Data Init
X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)
Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)

X_test = torch.tensor([5], dtype=torch.float32)
n_samples, n_features = X.size()    # n_samples = 4, n_features = 1
print(n_samples, n_features)

input_size = n_features
output_size = n_features
model = nn.Linear(input_size, output_size)

print("Prediction before training: f(5) = {}".format(model(X_test).item()))

# Training 
learning_rate = 0.01
n_iters = 100

# This is a callable function
loss = nn.MSELoss()

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for epoch in range(n_iters):
    # prediction
    y_pred = model(X)
    
    # loss
    l = loss(Y, y_pred)

    # gradients = backward pass
    l.backward()    # dl/dw
    
    # update weights
    optimizer.step()

    # zero gradients
    '''
        Whenever we call backward it will write our gradients and accumualte them in the `w.grad`
    '''
    optimizer.zero_grad()

    if epoch % 1 == 0:
        [w, b] = model.parameters()
        print("Epoch {0}: w = {1}, loss={2}".format(epoch+1, w[0][0], l))
    
print("Prediction after training: f(5) = {}".format(model(X_test).item()))
```



# 2. `torch.nn.MSELoss()`: Construct loss
<font color="orange" size="4">

- `loss(Y, y_pred)`: is a callable function.  

</font>

## 2.1. 使用实例
```python
import torch.nn as nn


# This `loss` is a callable function
loss = nn.MSELoss()

for epoch in range(n_iters):
    ...
    l = loss(Y, y_pred)
    ...
```

# 3. `torch.optim.SGD([w], lr)` -- Construct optimizer
<font color="orange" size="4">

- `optimizer.step()`: 更新权重(`weights`)或参数(`parameters`)
- `optimizer.zero_grad()`: 将梯度设置为零，防止梯度(`gradients`)累积

</font>

## 3.1. 使用实例
```python
optimizer = torch.optim.SGD([w], lr=learning_rate)

for epoch in range(n_iters):
    ...
    # update weights
    optimizer.step()

    # zero gradients
    optimizer.zero_grad()
    ...
```


# 4. `torch.nn.Linear(X_test)`
<font color="orange" size="4">

- `nn.Linear(input_size, output_size)`: model 初始化
- `model(X)`: 计算 prediction

</font>

## 4.1. 使用实例
```python
import torch
import torch.nn as nn


X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)
Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)

n_samples, n_features = X.size()
input_size = n_features
output_size = n_features

model = nn.Linear(input_size, output_size)

for epoch in range(n_iters):
    y_pred = model(X)
    ...
```



# 5. Pytorch 自定义 `LinearRegression` model object 
<font color="gree" size="4">

- `torch.nn.Module`: 常用语自定义 `PyTorch model`

</font>

```python
import torch
import torch.nn as nn


class LinearRegression(nn.Module):

    def __init__(self, input_dim, output_dim):
        super(LinearRegression, self).__init__()
        # define layers
        self.lin = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.lin(x)

model = LinearRegression(input_size, output_size)
```