<!--
 * @Descripttion: 
 * @version: 
 * @Author: sch
 * @Date: 2022-03-02 13:25:20
 * @LastEditors: sch
 * @LastEditTime: 2022-03-02 14:25:40
-->
# Softmax and Cross Entropy
<font color="red" size="4">

1. `Softmax` function: make the output of nn between 0 and 1 (`possibility`)
2. `Cross-Entropy` loss: 描述分类模型的performance, outputs are between 0 and 1

</font>

# 1. `Softmax function`
<font color="orange" size="4">

定义
---
$$ S(y_i) = e^{y_i} / \sum{e^{y_j}} $$

</font>


## 1.1. `numpy` 实现
```python
import torch
import torch.nn as nn
import numpy as np


def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)


x = np.array([2.0, 1.0, 0.1])
outputs = softmax(x)
print("softmax numpy: ", outputs)
```

Output:
```shell
softmax numpy:  [0.65900114 0.24243297 0.09856589]
```


## 1.2. PyTorch 实现
```python
import torch 
import torch.nn as nn

x = torch.tensor([2.0, 1.0, 0.1])
outputs = torch.softmax(x, dim=0)
print(outputs)
```


# 2. `Cross-Entropy` -- Onehot Encode
## 2.1. `numpy` 实现
```python
import torch
import numpy as np


def cross_entropy(actual, predicted):
    loss = -np.sum(actual * np.log(predicted))
    return loss     # / float(predicted.shape[0])


# y must be one hot code
# if class 0: [1 0 0]
# if class 1: [0 1 0]
# if class 2: [0 0 1]   
Y = np.array([1, 0, 0])


# y_pred has probabilities
Y_pred_good = np.array([0.7, 0.2, 0.1])
Y_pred_bad = np.array([0.1, 0.3, 0.6])
l1 = cross_entropy(Y, Y_pred_good)
l2 = cross_entropy(Y, Y_pred_bad)
print("Loss1 numpy (good): ", l1)
print("Loss2 numpy (bad): ", l2)
```

Output:
```shell
Loss1 numpy (good):  0.35667494393873245
Loss2 numpy (bad):  2.3025850929940455
```

## 2.2. `torch` 实现
