# Pipeline

<font color="orange" size="4">

1. Design model ( input, output size, forward pass )
2. Construct `loss` and `optimizer`
3. Training loop
      - `forward pass`: compute the prediction
      - `backward pass`: gradients 
      - `updates weights`

</font>


# 1. Linear Regression Demo
<font color="red" size="4">

Note
----
1. `torch.view(-1, 1)`: For `y`(the target we want to predict), We want put each value in one row, and the whole shape has only one column.
2. `predicted = model(X).detach().numpy()`: 当 `torch.tensor -> np.ndarray` 时，prevent this operation from being tracked in our graph and computational graph

</font>

```python
import torch
import torch.nn as nn
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt

# 0) prepare data
X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=0)

X = torch.from_numpy(X_numpy.astype(np.float32))
y = torch.from_numpy(y_numpy.astype(np.float32)) 
# We want to put each value in one row, and the whole shape has only one column.
y = y.view(y.size()[0], 1)

n_samples, n_features = X.size()


# 1) model
input_size = n_features
output_size = 1
model = nn.Linear(input_size, output_size)

# 2) loss and optimizer
learning_rate = 0.01
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# 3) training loop
num_epochs = 100
for epoch in range(1, num_epochs+1):
    # forward pass and loss
    y_pred = model(X)
    loss = criterion(y_pred, y)

    # back pass
    loss.backward()

    # update
    optimizer.step()

    # make gradient zeros
    optimizer.zero_grad()

    if ( epoch % 10 == 0 ):
        [w, b] = model.parameters()
        print("Epoch {0}: weights = {1}, loss = {2}".format(epoch, w[0][0], loss))

# plot
# prevent this operation from being tracked in our graph and computational graph
predicted = model(X).detach().numpy()
plt.plot(X_numpy, y_numpy, 'ro')
plt.plot(X_numpy, predicted, 'b')
plt.show()

```


```python
import torch 
import torch.nn as nn
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt


# 0) data preparation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1,
                                            noise=20, random_state=0)
X = torch.from_numpy(X_numpy).float()
y = torch.from_numpy(y_numpy).float()
y = y.view(-1, 1)

n_samples, n_features = X.shape


# 1）model
input_size = n_features
output_size = 1
model = nn.Linear(input_size, output_size)


# 2) loss and optimizer
learning_rate = 0.01
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)


# 3) training loop
num_epochs = 1000

for epoch in range(num_epochs):
    # forward
    y_pred = model(X)
    loss = criterion(y_pred, y)

    # backward pass
    loss.backward()

    # upodate weights
    optimizer.step()

    # make gradient zeros 0
    optimizer.zero_grad()

    if ( (epoch + 1) % 10 == 0 ):
        [w, b] = model.parameters()
        print("Epoch {0}: weights = {1}, loss={2}".format(epoch, w[0][0], loss))


with torch.no_grad():
    y_pred = model(X).numpy().ravel()
    plt.plot(y_pred, y_numpy, "ro")
    plt.plot([-100, 100], [-100, 100], "b")
    plt.show()
```