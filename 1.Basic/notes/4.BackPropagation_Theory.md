<!--
 * @Descripttion: 
 * @version: 
 * @Author: sch
 * @Date: 2022-02-28 10:10:41
 * @LastEditors: sch
 * @LastEditTime: 2022-02-28 10:28:36
-->
# Back Propagation Theory -- `Computation Graph`
<font color="red" size="4">

`Back Propagation`: 本质上就是在不断地训练loops中更新权重(`weights`)

</font>

# 1. `Back Propagation` 步骤
<font color="orange" size="4">

1. `Forward pass`: Compute Loss
2. `Compute local gradients`
3. `Backward pass`: Computing `dLoss / dWeights`(weights or parameters) using chain rules

</font>