{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch `tensor.backward()` fucntion\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. This post examines some `tensor.backward()` function examples about the `autograd (Automatic Differentiation)` package of PyTorch.\n",
    "2. As you already know, if you want to compute all the derivatives of a tensor, you can call `backward()` on it. (`tensor.backward()`)\n",
    "3. The `torch.tensor.backward()` relies on the autograd function `torch.autograd.backward()` that computes the `sum of gradients (without returning it) of given tensors with respect to the graph leaves`.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A first Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Example 的描述\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. Give a matrix $x$,\n",
    "$$\\begin{aligned}\n",
    "x :=\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "x_3 & x_4\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "2. and another matrix y is defined as\n",
    "$$\\begin{aligned}\n",
    "y := \n",
    "x + 2 = \n",
    "\\begin{bmatrix}\n",
    "x_1 + 2  &  x_2 + 2 \\\\\n",
    "x_3 + 2  &  x_4 + 2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "3 & 3 \\\\\n",
    "3 & 3 \n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "3. We define z as:\n",
    "    - <font color=\"red\">Note: `*` -- entry-wise multiplication</font>\n",
    "$$\\begin{aligned}\n",
    "z = y * y * 3 = \n",
    "3 * \n",
    "\\begin{bmatrix}\n",
    "y_1 & y_2 \\\\ \n",
    "y_3 & y_4\n",
    "\\end{bmatrix} * \n",
    "\\begin{bmatrix}\n",
    "y_1 & y_2 \\\\ \n",
    "y_3 & y_4\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "3y_1^2 & 3y_2^2 \\\\ \n",
    "3y_3^2 & 3y_4^2\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "4. Finally, we define out as:\n",
    "$$\\begin{aligned}\n",
    "out = \\frac{1}{4}(3y_1^2 + 3y_2^2 + 3y_3^2 + 3y_4^2)\n",
    "\\end{aligned}$$\n",
    "5. The value of derivative of out with respect with $x_2$\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial out}{\\partial x_2}\n",
    "&= \\frac{\\partial}{\\partial x_2} \\left( \\frac{1}{4}(3y_1^2+3y_2^2+3y_3^2+3y_4^2) \\right) \\\\\n",
    "&= 0 + \\frac{3}{4}\\frac{\\partial}{\\partial x_2}y_2^2 + 0 + 0 \\\\\n",
    "&= \\frac{3}{4}\\frac{\\partial}{\\partial x_2}(x_2 + 2)^2 \\\\\n",
    "&= \\frac{3}{2}(x_2 + 2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. $out$ contains `a single real value`(`scaler`). This value is the result of `scalar function`(In the case, the `mean` function).\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Code for example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. The `grad` attribute of `x` is None by default and becomes a tensor the first time a call to `out.backward()` computes gradients for self($\\frac{\\partial out}{\\partial x}$).\n",
    "2. The `grad` attribute will then contain the gradients computed and future calls to `backward()` will accumulate (add) gradients into it. \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "out (a scaler) =  27.0\n",
      "dout/dx =  tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "### Part I. device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "### Part II. example code\n",
    "## Step 1. define how to calculate\n",
    "x = torch.ones(\n",
    "            (2, 2),\n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "# y.grad_fn = AddBackward\n",
    "y = x + 2\n",
    "# z.grad_fn = MultiBackward\n",
    "z = y * y  * 3  # element-wise multiplcation\n",
    "# out.grad_fn = MeanBackward\n",
    "output = z.mean()\n",
    "print(\"out (a scaler) = \", output.item())\n",
    "\n",
    "## Step 2. calculate the `derivatives of out with respect to x`: dout/dx\n",
    "output.backward()\n",
    "print(\"dout/dx = \", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. `x.grad` will accumulate if not `optimizer.zero_grad()` in training loop\n",
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. The `grad` attribute will then contain the gradients computed and future calls to `backward()` will accumulate (add) gradients into it. \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First:\n",
      "\tdout/dx =  tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "\n",
      "Second:\n",
      "\tdout/dx =  tensor([[9., 9.],\n",
      "        [9., 9.]])\n"
     ]
    }
   ],
   "source": [
    "# Step 1. init torch.tensor x\n",
    "x = torch.ones(\n",
    "            (2, 2),\n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "def calculate_output(x: torch.tensor):\n",
    "    y = x + 2\n",
    "    z = y * y * 3   # element-wise multiplication\n",
    "    output = z.mean()\n",
    "    output.backward()\n",
    "\n",
    "\n",
    "# Step 2. first time\n",
    "calculate_output(x=x)\n",
    "print(\"First:\\n\\tdout/dx = \", x.grad)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Step 3. second time\n",
    "calculate_output(x=x)\n",
    "print(\"Second:\\n\\tdout/dx = \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workdir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba638ad70ccaeb6ccd331f5570d9b10b0c0ed5153ede80b1237556805ea6ecaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
