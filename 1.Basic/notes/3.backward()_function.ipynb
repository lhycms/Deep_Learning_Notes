{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch `tensor.backward()` fucntion\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. This post examines some `tensor.backward()` function examples about the `autograd (Automatic Differentiation)` package of PyTorch.\n",
    "2. As you already know, if you want to compute all the derivatives of a tensor, you can call `backward()` on it. (`tensor.backward()`)\n",
    "3. <font color=\"red\">The `torch.tensor.backward()` relies on the autograd function `torch.autograd.backward()` that computes the `sum of gradients (without returning it) of given tensors with respect to the graph leaves（all leaf nodes/tensor）`.\n",
    "    - 例如在下图中，当`loss.backward()`之后，可以得到\n",
    "        1. `x.grad`\n",
    "        2. `W.grad`\n",
    "        3. `b.grad`\n",
    "        4. `y.grad`\n",
    "        5. `z.grad` is not a leaf tensor\n",
    "</font>\n",
    "\n",
    "![computation_graph](./pics/computational_graph.png)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A first Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Example 的描述\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. Give a matrix $x$,\n",
    "$$\\begin{aligned}\n",
    "x :=\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "x_3 & x_4\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "2. and another matrix y is defined as\n",
    "$$\\begin{aligned}\n",
    "y := \n",
    "x + 2 = \n",
    "\\begin{bmatrix}\n",
    "x_1 + 2  &  x_2 + 2 \\\\\n",
    "x_3 + 2  &  x_4 + 2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "3 & 3 \\\\\n",
    "3 & 3 \n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "3. We define z as:\n",
    "    - <font color=\"red\">Note: `*` -- entry-wise multiplication</font>\n",
    "$$\\begin{aligned}\n",
    "z = y * y * 3 = \n",
    "3 * \n",
    "\\begin{bmatrix}\n",
    "y_1 & y_2 \\\\ \n",
    "y_3 & y_4\n",
    "\\end{bmatrix} * \n",
    "\\begin{bmatrix}\n",
    "y_1 & y_2 \\\\ \n",
    "y_3 & y_4\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "3y_1^2 & 3y_2^2 \\\\ \n",
    "3y_3^2 & 3y_4^2\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "4. Finally, we define out as:\n",
    "$$\\begin{aligned}\n",
    "out = \\frac{1}{4}(3y_1^2 + 3y_2^2 + 3y_3^2 + 3y_4^2)\n",
    "\\end{aligned}$$\n",
    "5. The value of derivative of out with respect with $x_2$\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial out}{\\partial x_2}\n",
    "&= \\frac{\\partial}{\\partial x_2} \\left( \\frac{1}{4}(3y_1^2+3y_2^2+3y_3^2+3y_4^2) \\right) \\\\\n",
    "&= 0 + \\frac{3}{4}\\frac{\\partial}{\\partial x_2}y_2^2 + 0 + 0 \\\\\n",
    "&= \\frac{3}{4}\\frac{\\partial}{\\partial x_2}(x_2 + 2)^2 \\\\\n",
    "&= \\frac{3}{2}(x_2 + 2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. $out$ contains `a single real value`(`scaler`). This value is the result of `scalar function`(In the case, the `mean` function).\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Code for example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. The `grad` attribute of `x` is None by default and becomes a tensor the first time a call to `out.backward()` computes gradients for self($\\frac{\\partial out}{\\partial x}$).\n",
    "2. The `grad` attribute will then contain the gradients computed and future calls to `backward()` will accumulate (add) gradients into it. \n",
    "3. Alternative option for `tensor.backward()`:\n",
    "    - `torch.autograd.grad(outputs=out, inputs=x)`\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "out (a scaler) =  27.0\n",
      "dout/dx =  tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "### Part I. device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "### Part II. example code\n",
    "## Step 1. define how to calculate\n",
    "x = torch.ones(\n",
    "            (2, 2),\n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "# y.grad_fn = AddBackward\n",
    "y = x + 2\n",
    "# z.grad_fn = MultiBackward\n",
    "z = y * y  * 3  # element-wise multiplcation\n",
    "# out.grad_fn = MeanBackward\n",
    "output = z.mean()\n",
    "print(\"out (a scaler) = \", output.item())\n",
    "\n",
    "## Step 2. calculate the `derivatives of out with respect to x`: dout/dx\n",
    "output.backward()\n",
    "print(\"dout/dx = \", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. `x.grad` will accumulate if not `optimizer.zero_grad()` in training loop\n",
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. The `grad` attribute will then contain the gradients computed and future calls to `backward()` will accumulate (add) gradients into it. \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First:\n",
      "\tdout/dx =  tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "\n",
      "Second:\n",
      "\tdout/dx =  tensor([[9., 9.],\n",
      "        [9., 9.]])\n"
     ]
    }
   ],
   "source": [
    "# Step 1. init torch.tensor x\n",
    "x = torch.ones(\n",
    "            (2, 2),\n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "def calculate_output(x: torch.tensor):\n",
    "    y = x + 2\n",
    "    z = y * y * 3   # element-wise multiplication\n",
    "    output = z.mean()\n",
    "    output.backward()\n",
    "\n",
    "\n",
    "# Step 2. first time\n",
    "calculate_output(x=x)\n",
    "print(\"First:\\n\\tdout/dx = \", x.grad)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Step 3. second time\n",
    "calculate_output(x=x)\n",
    "print(\"Second:\\n\\tdout/dx = \", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A neural networks example\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. <font color=\"red\">Neural networks use `backpropagation algorithm`: neural network parameters (`model weights`) are adjusted according to `the gradient of the loss function with respect to the given parameters`.\n",
    "    - 我们需要求的是 `损失函数相对于权重的导数`</font>\n",
    "2. PyTorch has `torch.autograd` as built-in engine to compute those gradients.\n",
    "3. The engine supports `automatic computation of gradients` for any `computational graph`.\n",
    "4. Consider the simplest one-layer neural network, with input $x$, parameters $W$ and $b$, and some `loss function`.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Code for simplest neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0631, -0.0755, -0.1221,  0.2267,  0.2879,  0.0203,  0.2067,  0.1933])\n",
      "tensor([[0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980],\n",
      "        [0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980],\n",
      "        [0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980],\n",
      "        [0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980],\n",
      "        [0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980],\n",
      "        [0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980],\n",
      "        [0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980],\n",
      "        [0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "         0.0980]])\n",
      "tensor([0.0113, 0.0900, 0.0055, 0.0827, 0.0002, 0.0205, 0.0006, 0.0100, 0.0131,\n",
      "        0.0980])\n",
      "None\n",
      "tensor([ 0.2058, -0.2192,  0.2837, -0.1568,  0.6092,  0.1357,  0.5125,  0.2193,\n",
      "         0.1891, -0.3874])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1776/1324917252.py:41: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(z.grad)\n"
     ]
    }
   ],
   "source": [
    "### Step 1. Data Preparation\n",
    "# input tensor\n",
    "x = torch.ones(\n",
    "                8,\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True,\n",
    "                device=device,\n",
    "                )\n",
    "# output tensor\n",
    "y = torch.zeros(\n",
    "                10,\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True,\n",
    "                device=device,\n",
    "                )\n",
    "# weights\n",
    "W = torch.randn(\n",
    "                (8, 10),\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True,\n",
    "                device=device,\n",
    "                )\n",
    "# bias vector\n",
    "b = torch.randn(10,\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True,\n",
    "                device=device,\n",
    "                ) \n",
    "\n",
    "\n",
    "### Step 2. Calculate\n",
    "z = torch.matmul(x, W) + b # output\n",
    "z.requires_grad_(True)\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "### Step 3. \n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "print(W.grad)\n",
    "print(b.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Computatioal Graph\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. Some node in graph:\n",
    "    - $x$: input\n",
    "    - $W$: model weight\n",
    "    - $b$: model bias\n",
    "    - $y$: real result\n",
    "    - $z$: result predicted by model (is not a leaf node/tensor)\n",
    "2. We can only obtain the grad properties for the <font color=\"red\">leaf nodes/tensor</font> of the computational graph which have <font color=\"red\">requires_grad</font> property set to True. Calling grad on non-leaf nodes will elicit a warning message:\n",
    "   ```bash\n",
    "   UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward().\n",
    "   ```\n",
    "</font>\n",
    "\n",
    "![computation_graph](./pics/computational_graph.png)\n",
    "\n",
    "<font color=\"coral\" size=\"4\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. Note that $loss$ is a scalar output. Applying `loss.backward()` directly on $loss$ (with no arguments) is not a problem because `loss represents a unique output and it is unambiguous to take its derivatives with respect to each variable in` $x$ / $W$ / $b$ / $y$. \n",
    "2. The situation changes when trying to call `backward()` on a `non-scalar output`? \n",
    "    - For example, consider the 10-entries tensor $z$. When calling `z.backward()` on it, what do you expect `x.grad` to be? We will address this problem in the following sections.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vector-Jacobian product\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. In general, `torch.autograd` is an engine for computing `vector-Jacobian products`\n",
    "   1. If `computational graph` is like below:\n",
    "      - $y = Y(x)$\n",
    "      - $l = L(y)$\n",
    "![cg_1](./pics/cg_1.png)\n",
    "   2. According to `rule of chain`:\n",
    "      - $j$ 最大为 $y$ 所在层神经元的个数\n",
    "      - $i$ 最大为 $x$ 所在层神经元的个数\n",
    "      - <font color=\"red\">$l$ is a `scalar`</font>, so $\\frac{\\partial l}{\\partial y_i}$ 的所有项组成一个 `vector` -- $v$\n",
    "      - $\\frac{\\partial y_i}{\\partial x_j}$ 的所有项组成一个 `Jacobian matrix` -- $J$\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial x_j}=\n",
    "\\frac{\\partial l}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial x_j}\n",
    "\\end{aligned}$$\n",
    "2. the product $J^T\\cdot\\vec{v}$ where $\\vec{v}$ is any vector and\n",
    "$$\\begin{aligned}\n",
    "J^T \\cdot v = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_n}{\\partial x_1} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_m} & \\cdots & \\frac{\\partial y_n}{\\partial x_m}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial l}{\\partial y_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial y_m}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial l}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Demo 1\n",
    "### 3.1.1. Computational graph\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. $y_i = x_i + 2$\n",
    "2. $z_i = 3y_i^2$\n",
    "3. $out = mean(z)$\n",
    "\n",
    "</font>\n",
    "\n",
    "![cg_2](./pics/cg_2.png)\n",
    "\n",
    "### 3.1.2. Procedure\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. According to `rule of chain`:\n",
    "    - 因为 $out$ 是一个标量，因此所有的 $\\frac{\\partial out}{\\partial x}$ 项构成一个 `vector` -- $v$\n",
    "    - 所有的 $\\frac{\\partial z}{\\partial y}$ 项构成 `Jacobian Matrix` -- $J_1$\n",
    "    - 所有的 $\\frac{\\partial y}{\\partial x}$ 项构成 `Jacobian Matrix` -- $J_2$\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial out}{\\partial x} = \n",
    "\\frac{\\partial out}{\\partial z} \\cdot\n",
    "\\frac{\\partial z}{\\partial y} \\cdot\n",
    "\\frac{\\partial y}{\\partial x}\n",
    "\\end{aligned}$$\n",
    "2. 求 $J_1$\n",
    "$$\\begin{aligned}\n",
    "J_1^T = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial z_1}{\\partial y_1} & \\cdots & \\frac{\\partial z_4}{\\partial y_1} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial z_1}{\\partial y_4} & \\cdots & \\frac{\\partial z_4}{\\partial y_4}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "6 & 0 & 0 & 0 \\\\\n",
    "0 & 6 & 0 & 0 \\\\\n",
    "0 & 0 & 6 & 0 \\\\\n",
    "0 & 0 & 0 & 6 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "3. 求 $J_2$\n",
    "$$\\begin{aligned}\n",
    "J_2^T = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_4}{\\partial x_1} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_4} & \\cdots & \\frac{\\partial z_4}{\\partial y_4}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{4} & 0 & 0 & 0 \\\\\n",
    "0 & \\frac{1}{4} & 0 & 0 \\\\\n",
    "0 & 0 & \\frac{1}{4} & 0 \\\\\n",
    "0 & 0 & 0 & \\frac{1}{4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "4. 求 $v$\n",
    "$$\\begin{aligned}\n",
    "v = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial l}{\\partial z_1} \\\\\n",
    "\\frac{\\partial l}{\\partial z_2} \\\\\n",
    "\\frac{\\partial l}{\\partial z_3} \\\\\n",
    "\\frac{\\partial l}{\\partial z_4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "5. 求 $J_1^TJ_2^Tv$\n",
    "   ```python\n",
    "   # 代码求 J_1^TJ_2^Tv\n",
    "   out.backward()\n",
    "   result = x.grad()\n",
    "   ```\n",
    "$$\\begin{aligned}\n",
    "J_1^TJ_2^Tv = \n",
    "\\begin{bmatrix}\n",
    "\\frac{3}{2}\\frac{\\partial l}{\\partial z_1} \\\\\n",
    "\\frac{3}{2}\\frac{\\partial l}{\\partial z_2} \\\\\n",
    "\\frac{3}{2}\\frac{\\partial l}{\\partial z_3} \\\\\n",
    "\\frac{3}{2}\\frac{\\partial l}{\\partial z_4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Demo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6912, 0.3989, 0.8603])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(\n",
    "            3,\n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "y = x + 2\n",
    "\n",
    "\n",
    "# y.backward()\n",
    "# Abort Error : \n",
    "#       RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "\n",
    "# v: `vector` in `Jacobaian Matrix * vector`\n",
    "v = torch.rand(\n",
    "            3,\n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Demo 3\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. Let $x=[x_1, x_2]$ and define $y$ as \n",
    "$$\\begin{aligned}\n",
    "[y_1, y_2, y_3] := [x_1^2, x_1^2+5x_2^2, 3x_2]\n",
    "\\end{aligned}$$\n",
    "2. In this case, `tansposed Jacobina Matrix` is \n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_1} \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_2} \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "2x_1 & 2x_1 & 0 \\\\\n",
    "0 & 10x_2 & 3\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "3. If $x=[1, 2]$, choose vector $v = [1, 1, 1]$\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "2 & 2 & 0 \\\\\n",
    "0 & 20 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "23 \n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y =  tensor([ 1., 21.,  6.], grad_fn=<CopySlices>)\n",
      "x.grad =  tensor([ 4., 23.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "            [1, 2],\n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "y = torch.empty(3)\n",
    "y[0] = x[0]**2\n",
    "y[1] = x[0]**2 + 5*x[1]**2\n",
    "y[2] = 3*x[1]\n",
    "print(\"y = \", y)\n",
    "\n",
    "v = torch.ones(\n",
    "            3, \n",
    "            device=device,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "y.backward(v)\n",
    "print(\"x.grad = \", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. The general case\n",
    "<font color=\"steelblue\" size=\"4\">\n",
    "\n",
    "1. We have seen - and it is also shown on the `official autograd page` -- that if you have a function $y = f(x)$\n",
    "2. But what happens when v  is not a simple vector? See `Demo 1.`\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workdir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba638ad70ccaeb6ccd331f5570d9b10b0c0ed5153ede80b1237556805ea6ecaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
