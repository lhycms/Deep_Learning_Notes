<!--
 * @Descripttion: 
 * @version: 
 * @Author: sch
 * @Date: 2022-02-27 21:03:33
 * @LastEditors: sch
 * @LastEditTime: 2022-02-27 22:22:47
-->
# Gradient Calculation With Autograd

<font color="red" size="4">

PyTorch 为我们做了封装，可以通过调包快速计算梯度(`gradient`)。

</font>


# 1. `tensor.requires_grad=True`（`default=True`）：用于计算梯度(gradient)
<font color="orange" size="4">

- If we want to calculate the gradient of some function with respect to X, then what we have to do is we must specify the argument `requires_grad = True`
- Whenever we do operations with this tensor, PyTorch will create a so-called `computational graph` for us.
    1. There are inputs and outputs for each node in `computational graph`.
    2. `Forward pass`
    3. `Back propagation`: Calculate the gradient
- `Back propagation` 的种类 (体现在 tensor 的 `grad_fn` 属性)
    1. `grad_fn=<AddBackward0>`
    2. `grad_fn=<MulBackward0>`
    3. `grad_fn=<MeanBackward0>`

</font>

```python
import torch 


x = torch.randn(3, requires_grad=True)
print(x)

# In output of below expression, we can see `grad_fn` attribute = `<AddBackward0>`, it's because `y = x + 2` (进行的是加法)
y = x + 2
print(y)

z = y * y * 2
print(z)

z = z.mean()
print(z)
```
Output:
```shell
tensor([-1.2301,  0.5498, -1.7590], requires_grad=True)
tensor([0.7699, 2.5498, 0.2410], grad_fn=<AddBackward0>)
tensor([3.6553, 3.8712, 2.5367], grad_fn=<MulBackward0>)
tensor(3.3544, grad_fn=<MeanBackward0>)
```

<font color="red" size="4">

Note
----
1. In above output of code, we can see `grad_fn` attribute = `<AddBackward0>`, it's because `y = x + 2` (进行的是加法)

</font>


## 1.1. `z.backward()`: 计算梯度 `dz/dx` -- 针对 z 是标量
```python
import torch


x = torch.randn(3, requires_grad=True)
y = x + 2
z = y * y * 2
z = z.mean()
z.backward()    # dz/dx
print(x.grad)
```
Output:
```shell
tensor([2.4794, 2.8327, 4.9375])
```

<font color="red" size="4">

Note
----
1. 在上述例子中如果删除`z = z.mean()`，则会报错：`RuntimeError: grad can be implicitly created only for scalar outputs`: 说明 gradients can be created only for scaler outputs.

</font>

## 1.2. `z.backward(tensor)`: 计算梯度 `dz/dx` -- 针对 z 不是标量
```python
import torch 


x = torch.randn(3, requires_grad=True)
y = x + 2
z = y * y * 2

v = torch.tensor([0.1, 1.0, 0.01], dtype=torch.float32)
z.backward(v)   # dz/dx
print(x.grad)
```
Output:
```shell
tensor([0.6917, 9.0856, 0.0988])
```


# 2.
<font color="orange" size="4">

1. `x.requires_grad_(False)`
2. `x.detach()`: This will create a new tensor that doesn't require gradient.
3. `with torch.no_grad():`

</font>

## 2.1. `x.requires_grad_(False)`
```python
import torch


x = torch.randn(3, requires_grad=True)
print(x)

x.requires_grad_(False)
print(x)
```
Output:
```shell
tensor([-1.6066,  0.2085,  1.8512], requires_grad=True)
tensor([-1.6066,  0.2085,  1.8512])
```

## 2.2. `x.detach()`
```python
import torch 


x = torch.randn(3, requires_grad=True)
print(x)
y = x.detach()
print(y)
```
Output:
```shell
tensor([-0.4009, -2.1106, -0.5101], requires_grad=True)
tensor([-0.4009, -2.1106, -0.5101])
```

## 2.3. `with torch.no_grad():`
```python
import torch 


x = torch.randn(3, requires_grad=True)
print(x)
with torch.no_grad():
    y = x + 2
    print(y)
```
Output:
```shell
tensor([-0.8544, -0.4829,  0.3729], requires_grad=True)
tensor([1.1456, 1.5171, 2.3729])
```


# 3. `x.grad.zero_()`
<font color="red" size="4">

Note
----
1. `x.grad`（由`requires_grad=True`得来）是随着训练次数(training loop) 累积的
2. Before we do the next iteration and optimazition, we must `empty the gradient`

</font>

## 3.1. 不使用 `x.grad.zero_()`
```python
import torch 


weights = torch.ones(4, requires_grad=True)

for epoch in range(3):
    model_output = (weights * 3).sum()

    model_output.backward()

    print(weights.grad)
```
Output:
```shell
tensor([3., 3., 3., 3.])
tensor([6., 6., 6., 6.])
tensor([9., 9., 9., 9.])
```

## 3.2. 使用 `x.grad.zero_()`
```python
import torch 


weights = torch.ones(4, requires_grad=True)

for epoch in range(3):
    model_output = (weights * 3).sum()

    model_output.backward()

    print(weights.grad)

    weights.grad.zero_()
```
Output:
```shell
tensor([3., 3., 3., 3.])
tensor([3., 3., 3., 3.])
tensor([3., 3., 3., 3.])
```


# 4. `torch.optim.SGD(weights, lr)` -- stochastic gradient descent
```python
import torch


weights = torch.ones(4, requires_grad=True)

optimizer = torch.optim.SGD(weights, lr=0.01)
optimizer.step()    # do a optimization step
optimizer.zero_grad()   # 在每个training loop 中，weights 是累积的，本步将在每次loop结尾清空 gradient
```