<!--
 * @Descripttion: 
 * @version: 
 * @Author: sch
 * @Date: 2022-03-17 15:41:44
 * @LastEditors: sch
 * @LastEditTime: 2022-03-18 08:56:01
-->
# 1. 多层神经网络的构造
<font color="gree" size="4">

1. 全连接层：init weight and bias
2. Batch Normalization
3. Activation
4. Dropout: `model.train()` 和 `model.eval()`

</font>

```python
from turtle import forward
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# 0) Data preparation
X_numpy, y_numpy = datasets.make_regression(n_samples=1000, n_features=30,
                                            noise=20, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, 
                                                    test_size=0.2, random_state=0)                                        

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

X_train = torch.from_numpy(X_train).float()
X_test = torch.from_numpy(X_test).float()
y_train = torch.from_numpy(y_train).float()
y_test = torch.from_numpy(y_test).float()
X_train.requires_grad_()
y_train = y_train.view(-1, 1)
y_test = y_test.view(-1, 1)

n_samples, n_features = X_numpy.shape


# 1) model
class NNModel(nn.Module):
    
    DP_RATIO = 0.3

    def __init__(self, input_dim:int, hidden_dims_lst:list, output_dim:int,
                do_bn:bool, do_dp:bool):
        super(NNModel, self).__init__()

        self.do_dp = do_dp
        self.do_bn = do_bn
        self.activation = nn.ReLU()

        self.fcs, self.dps, self.bns = [], [], []
        hidden_dims_lst = [input_dim] + hidden_dims_lst
        
        self.bn_input = nn.BatchNorm1d(input_dim)

        for idx in range(1, len(hidden_dims_lst)):
            fc = nn.Linear(hidden_dims_lst[idx-1], hidden_dims_lst[idx])
            self._init_fc(fc)
            setattr(self, "fc{0}".format(idx), fc)
            self.fcs.append(fc)

            if do_bn:
                bn = nn.BatchNorm1d(hidden_dims_lst[idx])
                setattr(self, "bn{0}".format(idx), bn)
                self.bns.append(bn)
            
            if do_dp:
                dp = nn.Dropout(self.DP_RATIO)
                setattr(self, "dp{0}".format(idx), dp)
                self.dps.append(dp)
        
        self.output_layer = nn.Linear(hidden_dims_lst[-1], output_dim)
        self._init_fc(self.output_layer)
    

    def _init_fc(self, fc):
        nn.init.normal_(fc.weight, mean=0, std=0.1)
        nn.init.constant_(fc.bias, 0)


    def forward(self, x):
        x = self.bn_input(x)

        for idx in range(len(self.fcs)):
            x = self.fcs[idx](x)

            if self.do_bn:
                x = self.bns[idx](x)
            
            x = self.activation(x)

            if self.do_dp:
                x = self.dps[idx](x)

        x = self.output_layer(x)
        
        return x


model = NNModel(n_features, [30, 10], 1, do_bn=True, do_dp=True)


# 2) loss and optimizer
learning_rate = 0.01
criterion = nn.MSELoss()
optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate,
                            weight_decay=0.001)


# 3) Training loop
num_epochs = 2000
model.train()
loss_train_lst = []
loss_test_lst = []

for epoch in range(num_epochs):
    # forward
    y_pred = model(X_train)
    loss = criterion(y_pred, y_train)
    loss_train_lst.append(loss)

    y_pred_test = model(X_test)
    loss_test = criterion(y_pred_test, y_test)
    loss_test_lst.append(loss_test)

    # backward
    loss.backward()

    # update weights
    optimizer.step()

    # make gradient zero
    optimizer.zero_grad()


    if ( (epoch+1) % 100 == 0 ):
        print("Epoch {0}: loss = {1}".format(epoch+1, loss))


with torch.no_grad():
    model.eval()

    y_pred = model(X_train)
    plt.plot(y_pred, y_train, "bo")
    plt.show()

    y_pred = model(X_test)
    plt.plot(y_pred, y_test, "ro")
    plt.show()

    plt.plot([*range(len(loss_train_lst))], loss_train_lst)
    plt.show()
    plt.plot([*range(len(loss_test_lst))], loss_test_lst)
    plt.show()
```