{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "engaged-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-comment",
   "metadata": {},
   "source": [
    "# 1. 当你设置`requires_grad=True`后，每次对tensor进行运算，PyTorch都会自动生成计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "extreme-upper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# If we want torch help us to calculate gradient, we should set `requires_grad=True`.\n",
    "# Now whenever we do operation with tensor, pytorch will create so-called Computation Graph\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "celtic-jonathan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 5.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad_fn=<AddBackward0>\n",
    "y = x + 2 \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-findings",
   "metadata": {},
   "source": [
    "## computation graph for `y = x + 2` :\n",
    "\n",
    "- 运算是一个node\n",
    "- 数据也是一个node\n",
    "\n",
    "\n",
    "<img src=\"./picut/computation_graph.png\" height=\"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "nuclear-header",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18., 32., 50.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad_fn=<MulBackward0>\n",
    "z = y * y * 2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-january",
   "metadata": {},
   "source": [
    "## 1.2 计算gradient\n",
    "\n",
    "- 当loss为标量时，`loss.backward()`\n",
    "- 当loss不是标量时，`loss.backward(与loss维度相同的tensor)`\n",
    "- `loss.backward()`：计算x.grad属性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-directive",
   "metadata": {},
   "source": [
    "### 1.2.1 计算gradient -- scalar\n",
    "\n",
    "<img src=\"./picut/computation_graph_1.jpg\" height=\"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "grave-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.3333, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad_fn=<MeanBackward0>\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "desperate-domestic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.0000, 5.3333, 6.6667])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # dz / dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-affiliate",
   "metadata": {},
   "source": [
    "### 1.2.2 计算gradient -- vector\n",
    "\n",
    "- dl / dx = Jacobian_Matrix * dl / dy\n",
    "- we should identy dl/dy here\n",
    "<img src=\"./picut/jacobian_matrix.png\" height=\"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "assured-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: input\n",
    "# y: target\n",
    "# z: loss\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x + 2 \n",
    "z = y * y * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "swedish-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12., 24., 30.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1., 1.5, 1.5], dtype=torch.float32)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-remains",
   "metadata": {},
   "source": [
    "# 2. Three attributes\n",
    "- `x.requires_grad_(False)`: 设置`tensor.requires_grad`属性\n",
    "- `x.detach()`: 创造一个新的`tensor`，但`tensor.requires_grad=False`(从当前的计算图中分离出来)\n",
    "- `with torch.no_grad()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-isaac",
   "metadata": {},
   "source": [
    "## 2.1 `tensor.requires_grad_(False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "pacific-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5389,  1.3927, -0.4943], requires_grad=True)\n",
      "tensor([-0.5389,  1.3927, -0.4943])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-relation",
   "metadata": {},
   "source": [
    "## 2.2 `tensor.detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "alternative-decline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9340,  0.0264, -0.2288], requires_grad=True)\n",
      "tensor([ 1.9340,  0.0264, -0.2288])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-annex",
   "metadata": {},
   "source": [
    "## 2.3 `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "billion-saudi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2138,  0.1682, -0.7964], requires_grad=True)\n",
      "tensor([0.7862, 2.1682, 1.2036], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "clinical-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6641,  0.7710, -0.4929], requires_grad=True)\n",
      "tensor([1.3359, 2.7710, 1.5071])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-southeast",
   "metadata": {},
   "source": [
    "# 3. Before we do the next optimization step(SGD), we must empty the gradients\n",
    "\n",
    "\n",
    "- `tensor.grad.zero_()`: empty gradient of tensor。 注意：必须在`loss.backward()`后调用，否则会报错\n",
    "\n",
    "```python\n",
    "for epoch in range(100):\n",
    "    # 进行100步优化\n",
    "    loss.backward()\n",
    "    print(weights.grad)\n",
    "    ...\n",
    "    weights.grad.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-anaheim",
   "metadata": {},
   "source": [
    "### In the following example:\n",
    "\n",
    "If we run second iteration, the second backward will be called, \n",
    "will again add the values and write them into grad attribute\n",
    "so we need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "adjacent-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights * 3).sum()\n",
    "    # print(model_output)\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-australia",
   "metadata": {},
   "source": [
    "## 3.1 优化实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "lightweight-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# 初始化optimizer\n",
    "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
    "# optimizer运行一步\n",
    "optimizer.step()\n",
    "# 清空梯度，以免下次`loss.backward()`时，梯度叠加\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-index",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
